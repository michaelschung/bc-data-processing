{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/mchung/opt/anaconda3/lib/python3.8/site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mchung/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.0.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 904 kB 3.9 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 184 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /Users/mchung/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.25.9)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n",
      "Requirement already satisfied: lxml in /Users/mchung/opt/anaconda3/lib/python3.8/site-packages (4.5.2)\n",
      "Collecting geckodriver-autoinstaller\n",
      "  Downloading geckodriver_autoinstaller-0.1.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: geckodriver-autoinstaller\n",
      "Successfully installed geckodriver-autoinstaller-0.1.0\n",
      "Collecting chromedriver-py\n",
      "  Downloading chromedriver_py-88.0.4324.27-py3-none-any.whl (19.5 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.5 MB 18.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: chromedriver-py\n",
      "Successfully installed chromedriver-py-88.0.4324.27\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install selenium\n",
    "!pip3 install lxml\n",
    "!pip3 install geckodriver-autoinstaller\n",
    "!pip3 install chromedriver-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import HTTPError\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_lists(soup):\n",
    "\n",
    "    lists = []\n",
    "    list_count_dict = {}\n",
    "\n",
    "    if soup.find('a', text='More lists with this book...'):\n",
    "\n",
    "        lists_url = soup.find('a', text='More lists with this book...')['href']\n",
    "\n",
    "        source = urlopen('https://www.goodreads.com' + lists_url)\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "        lists += [' '.join(node.text.strip().split()) for node in soup.find_all('div', {'class': 'cell'})]\n",
    "\n",
    "        i = 0\n",
    "        while soup.find('a', {'class': 'next_page'}) and i <= 10:\n",
    "\n",
    "            time.sleep(2)\n",
    "            next_url = 'https://www.goodreads.com' + soup.find('a', {'class': 'next_page'})['href']\n",
    "            source = urlopen(next_url)\n",
    "            soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "            lists += [node.text for node in soup.find_all('div', {'class': 'cell'})]\n",
    "            i += 1\n",
    "\n",
    "        # Format lists text.\n",
    "        for _list in lists:\n",
    "            # _list_name = ' '.join(_list.split()[:-8])\n",
    "            # _list_rank = int(_list.split()[-8][:-2]) \n",
    "            # _num_books_on_list = int(_list.split()[-5].replace(',', ''))\n",
    "            # list_count_dict[_list_name] = _list_rank / float(_num_books_on_list)     # TODO: switch this back to raw counts\n",
    "            _list_name = _list.split()[:-2][0]\n",
    "            _list_count = int(_list.split()[-2].replace(',', ''))\n",
    "            list_count_dict[_list_name] = _list_count\n",
    "\n",
    "    return list_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shelves(soup):\n",
    "\n",
    "    shelf_count_dict = {}\n",
    "    \n",
    "    if soup.find('a', text='See top shelves‚Ä¶'):\n",
    "\n",
    "        # Find shelves text.\n",
    "        shelves_url = soup.find('a', text='See top shelves‚Ä¶')['href']\n",
    "        source = urlopen('https://www.goodreads.com' + shelves_url)\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "        shelves = [' '.join(node.text.strip().split()) for node in soup.find_all('div', {'class': 'shelfStat'})]\n",
    "        \n",
    "        # Format shelves text.\n",
    "        shelf_count_dict = {}\n",
    "        for _shelf in shelves:\n",
    "            _shelf_name = _shelf.split()[:-2][0]\n",
    "            _shelf_count = int(_shelf.split()[-2].replace(',', ''))\n",
    "            shelf_count_dict[_shelf_name] = _shelf_count\n",
    "\n",
    "    return shelf_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genres(soup):\n",
    "    genres = []\n",
    "    for node in soup.find_all('div', {'class': 'left'}):\n",
    "        current_genres = node.find_all('a', {'class': 'actionLinkLite bookPageGenreLink'})\n",
    "        current_genre = ' > '.join([g.text for g in current_genres])\n",
    "        if current_genre.strip():\n",
    "            genres.append(current_genre)\n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isbn(soup):\n",
    "    try:\n",
    "        isbn = re.findall(r'nisbn: [0-9]{10}' , str(soup))[0].split()[1]\n",
    "        return isbn\n",
    "    except:\n",
    "        return \"isbn not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isbn13(soup):\n",
    "    try:\n",
    "        isbn13 = re.findall(r'nisbn13: [0-9]{13}' , str(soup))[0].split()[1]\n",
    "        return isbn13\n",
    "    except:\n",
    "        return \"isbn13 not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_distribution(soup):\n",
    "    distribution = re.findall(r'renderRatingGraph\\([\\s]*\\[[0-9,\\s]+', str(soup))[0]\n",
    "    distribution = ' '.join(distribution.split())\n",
    "    distribution = [int(c.strip()) for c in distribution.split('[')[1].split(',')]\n",
    "    distribution_dict = {'5 Stars': distribution[0],\n",
    "                         '4 Stars': distribution[1],\n",
    "                         '3 Stars': distribution[2],\n",
    "                         '2 Stars': distribution[3],\n",
    "                         '1 Star':  distribution[4]}\n",
    "    return distribution_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pages(soup):\n",
    "    if soup.find('span', {'itemprop': 'numberOfPages'}):\n",
    "        num_pages = soup.find('span', {'itemprop': 'numberOfPages'}).text.strip()\n",
    "        return int(num_pages.split()[0])\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_first_published(soup):\n",
    "    year_first_published = soup.find('nobr', attrs={'class':'greyText'}).string\n",
    "    return re.search('([0-9]{3,4})', year_first_published).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(bookid):\n",
    "    pattern = re.compile(\"([^.-]+)\")\n",
    "    return pattern.search(bookid).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_book(book_id):\n",
    "    url = 'https://www.goodreads.com/book/show/' + book_id\n",
    "    source = urlopen(url)\n",
    "    soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    return {'book_id_title':        book_id, \n",
    "            'book_id':              get_id(book_id), \n",
    "            'book_title':                ' '.join(soup.find('h1', {'id': 'bookTitle'}).text.split()), \n",
    "            'isbn':                 get_isbn(soup),\n",
    "            'isbn13':               get_isbn13(soup),\n",
    "            'year_first_published': get_year_first_published(soup), \n",
    "            'author':               ' '.join(soup.find('span', {'itemprop': 'name'}).text.split()), \n",
    "            'num_pages':            get_num_pages(soup), \n",
    "            'genres':               get_genres(soup), \n",
    "            'shelves':              get_shelves(soup), \n",
    "            'lists':                get_all_lists(soup), \n",
    "            'num_ratings':          soup.find('meta', {'itemprop': 'ratingCount'})['content'].strip(), \n",
    "            'num_reviews':          soup.find('meta', {'itemprop': 'reviewCount'})['content'].strip(),\n",
    "            'average_rating':       soup.find('span', {'itemprop': 'ratingValue'}).text.strip(), \n",
    "            'rating_distribution':  get_rating_distribution(soup)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_books(books_directory_path):\n",
    "\n",
    "    books = []\n",
    "\n",
    "    for file_name in os.listdir(books_directory_path):\n",
    "        if file_name.endswith('.json') and not file_name.startswith('.') and file_name != \"all_books.json\":\n",
    "            _book = json.load(open(books_directory_path + '/' + file_name, 'r')) #, encoding='utf-8', errors='ignore'))\n",
    "            books.append(_book)\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--book_ids_path', type=str)\n",
    "# parser.add_argument('--output_directory_path', type=str)\n",
    "# parser.add_argument('--format', type=str, action=\"store\", default=\"json\",\n",
    "#                     dest=\"format\", choices=[\"json\", \"csv\"],\n",
    "#                     help=\"set file output format\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "book_ids_path = 'book_ids.txt'\n",
    "\n",
    "book_ids              = [line.strip() for line in open(book_ids_path, 'r') if line.strip()]\n",
    "# books_already_scraped =  [file_name.replace('.json', '') for file_name in os.listdir(args.output_directory_path) if file_name.endswith('.json') and not file_name.startswith('all_books')]\n",
    "books_to_scrape       = [book_id for book_id in book_ids]\n",
    "# condensed_books_path   = args.output_directory_path + '/all_books'\n",
    "\n",
    "for i, book_id in enumerate(books_to_scrape):\n",
    "    try:\n",
    "        print(str(datetime.now()) + ': Scraping ' + book_id + '...')\n",
    "#         print(str(datetime.now()) + ': #' + str(i+1+len(books_already_scraped)) + ' out of ' + str(len(book_ids)) + ' books')\n",
    "\n",
    "        book = scrape_book(book_id)\n",
    "        json.dump(book, open(args.output_directory_path + '/' + book_id + '.json', 'w'))\n",
    "\n",
    "        print('=============================')\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "books = condense_books(args.output_directory_path)\n",
    "if args.format == 'json':\n",
    "    json.dump(books, open(f\"{condensed_books_path}.json\", 'w'))\n",
    "elif args.format == 'csv':\n",
    "    json.dump(books, open(f\"{condensed_books_path}.json\", 'w'))\n",
    "    book_df = pd.read_json(f\"{condensed_books_path}.json\")\n",
    "    book_df.to_csv(f\"{condensed_books_path}.csv\", index=False, encoding='utf-8')\n",
    "    \n",
    "print(str(datetime.now()) + ' ' + script_name + f':\\n\\nüéâ Success! All book metadata scraped. üéâ\\n\\nMetadata files have been output to /{args.output_directory_path}\\nGoodreads scraping run time = ‚è∞ ' + str(datetime.now() - start_time) + ' ‚è∞')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
